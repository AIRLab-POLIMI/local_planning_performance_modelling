{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "import traceback\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import yaml\n",
    "import itertools\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "from os import path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, TheilSenRegressor, RANSACRegressor, HuberRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def printmd(*args):\n",
    "    display(Markdown(' '.join(map(str, args))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "local_planner_color = {'dwa': 'blue', 'arena': 'orange', 'teb': 'green'}\n",
    "local_planner_symbol = {'dwa': 'o', 'arena': '^', 'teb': 'x'}\n",
    "robot_model_color = {'hunter': 'cyan', 'turtle': 'green'}\n",
    "pd.options.display.width = 200\n",
    "pd.options.display.max_rows = 10000\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "plt.rcParams['figure.figsize'] = [3, 3]\n",
    "plt.rcParams['lines.linewidth'] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path.expanduser(\"~/ds/performance_modelling/output/local_planning/results.csv\"))\n",
    "with open(path.expanduser(\"~/ds/performance_modelling/output/local_planning/results_info.yaml\")) as results_info_file:\n",
    "    results_info = yaml.safe_load(results_info_file)\n",
    "\n",
    "df.rename(inplace=True, columns={\n",
    "    'collisionless_localization_update_absolute_translation_error_mean': 'absolute_translation_error',\n",
    "    'collisionless_localization_update_absolute_rotation_error_mean': 'absolute_rotation_error',\n",
    "    'collisionless_localization_update_normalized_relative_translation_error_mean': 'normalized_relative_translation_error',\n",
    "    'collisionless_localization_update_normalized_relative_rotation_error_mean': 'normalized_relative_rotation_error',\n",
    "    'localization_update_rate_mean': 'localization_update_rate',\n",
    "})\n",
    "\n",
    "# turn odometry_error into beta_1..4\n",
    "results_info['run_parameter_names'] += ['beta_1', 'beta_2', 'beta_3', 'beta_4']\n",
    "results_info['run_parameter_names'].remove('odometry_error')\n",
    "for i in range(0, 4):\n",
    "    df[f'beta_{i+1}'] = df['odometry_error'].apply(lambda x: eval(x)[i])\n",
    "del df['odometry_error']\n",
    "\n",
    "df.loc[df.robot_model == 'turtlebot3_waffle_performance_modelling', 'robot_model'] = 'turtle'\n",
    "\n",
    "run_parameters = [c for c in list(df.columns) if c in results_info['run_parameter_names']]\n",
    "run_parameters += ['max_steering_angle_deg']\n",
    "metrics_versions = [c for c in list(df.columns) if '_version' in c]\n",
    "everything_else = ['run_id', 'session_id', 'run_number', 'goal_index']\n",
    "metrics = [c for c in df.columns if c not in metrics_versions + run_parameters + everything_else]\n",
    "metrics_and_versions = [c for c in list(df.columns) if '_version' in c or c in metrics]\n",
    "\n",
    "cpu_time_metrics = [c for c in metrics if 'cpu_time' in c]\n",
    "max_memory_metrics = [c for c in metrics if 'max_memory' in c]\n",
    "\n",
    "# add useful parameters\n",
    "df['session_id'] =  df['run_id'].apply(lambda x:  x.split('_')[1]+'_'+x.split('_')[2]+'_'+x.split('_')[3])\n",
    "df['run_number'] =  df['run_id'].apply(lambda x:  int(x.split('_')[5]))\n",
    "df[max_memory_metrics] = df[max_memory_metrics]/1024**2\n",
    "df['max_steering_angle_deg'] = 90    # crea una nuova colonna e riempie le righe con il valore 90\n",
    "df[\"run_index_str\"] = df['run_index'].apply(lambda x: str(x))\n",
    "df['goal_index'] = df.environment_name + '_' + df.run_index_str\n",
    "\n",
    "# add metrics from existing ones\n",
    "df['average_velocity'] = df['trajectory_length'] / df['execution_time']\n",
    "df['success_rate'] = df['success_rate'] & (1 - df['collision_rate'])\n",
    "\n",
    "metrics += ['average_velocity']\n",
    "metrics_and_versions += ['average_velocity']\n",
    "\n",
    "min_execution_time_group_df = df.groupby([\"environment_name\", \"run_index\", \"success_rate\"])\n",
    "for (environment_name, run_index, success_rate), group_df in min_execution_time_group_df:\n",
    "    df.loc[(df.environment_name == environment_name) & (df.run_index == run_index) & (success_rate), 'min_execution_time'] = group_df.execution_time.min()\n",
    "df['norm_execution_time'] = df.execution_time / df.min_execution_time\n",
    "metrics += ['norm_execution_time']\n",
    "metrics_and_versions += ['norm_execution_time']\n",
    "df = df[df.pedestrian_number < 100] # consider only runs with less than 100 peds\n",
    "df = df[df.arena_new_timeout.notna() | (df.local_planner_node != \"arena\")] # discard arena runs without the param arena_new_timeout\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all():\n",
    "    printmd(\"## Number of Runs\")\n",
    "    printmd(f\"    {len(df.run_id.unique())}\")\n",
    "\n",
    "    printmd(\"## Run Parameters\")\n",
    "    for name in [run_parameter_name for run_parameter_name in run_parameters if 'localization_generator_' not in run_parameter_name]:\n",
    "        values = list(df[name].unique())\n",
    "        printmd(f\"    {name:<70}\", sorted(values))\n",
    "\n",
    "    printmd(\"## Metrics\")\n",
    "    for name in metrics_and_versions:\n",
    "        if name in metrics_versions:\n",
    "            if len(df[name].unique()) == 1:\n",
    "                printmd(f\"    {name:<70} {sorted(df[name].unique())}\")\n",
    "            else:\n",
    "                printmd(f\"<code><font style='background-color:yellow;font-family:monospace'>{name:<70}{sorted(df[name].unique())} </font></code> ⚠️\")\n",
    "        else:\n",
    "            printmd(f\"    {name:<70} min: {df[name].min(skipna=True):10.4g} {'avg':>15}: {df[name].mean(skipna=True):10.4g} {'max':>15}: {df[name].max(skipna=True):10.4g} {'nan':>15}: {sum(df[name].isna()):10.4g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
